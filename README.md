
```markdown
# ğŸ§  Persistent Knowledge RAG Agent (v1.0.0)

A framework-free, disk-persistent Retrieval-Augmented Generation (RAG) system
built **from first principles**.

This project implements a **long-term knowledge agent** that can:
- Ingest files (PDF / TXT / MD)
- Store knowledge persistently on disk
- Retrieve relevant past information using embeddings
- Generate **grounded, confidence-aware answers**
- Improve over time as memory grows

> âš ï¸ This is **not** a LangChain demo.  
> This is a systems-level RAG implementation designed.
---

The agent behaves like a **long-term brain**, not a planner or executor.

```

Input â†’ Retrieve memory â†’ Answer â†’ Persist knowledge

```

---

## ğŸ§± High-Level Architecture

File Ingestion  
â†“  
Persistent Memory (metadata.json)  
â†“  
Chunking Engine (chunks.json)  
â†“  
Embedding Store (embeddings.json)  
â†“  
Retriever (similarity + threshold)  
â†“  
Grounded Answer Generator


```

```

All state lives **on disk**, survives restarts, and can be rebuilt deterministically.

---

## ğŸ“‚ Project Structure
silver-system-RAG/  
â”œâ”€â”€ main.py # Unified CLI entry point  
â”œâ”€â”€ ingest/  
â”‚ â”œâ”€â”€ file_ingestor.py # File â†’ memory â†’ chunks â†’ embeddings  
â”‚ â””â”€â”€ chunker.py # Deterministic chunking engine  
â”œâ”€â”€ embeddings/  
â”‚ â”œâ”€â”€ embedder.py # Embedding model abstraction  
â”‚ â””â”€â”€ embedding_store.py # Disk-backed vector store  
â”œâ”€â”€ retrieval/  
â”‚ â””â”€â”€ retriever.py # Similarity search + thresholding  
â”œâ”€â”€ llm/  
â”‚ â””â”€â”€ answer_generator.py # Grounded answer generation  
â”œâ”€â”€ memory/  
â”‚ â””â”€â”€ metadata_store.py # Append-only source-of-truth memory  
â”œâ”€â”€ data/  
â”‚ â”œâ”€â”€ metadata.json # Persistent memories  
â”‚ â”œâ”€â”€ chunks.json # Derived chunks (disposable)  
â”‚ â””â”€â”€ embeddings.json # Stored embeddings  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ README.md
```

````

## ğŸ§  Phase-by-Phase Breakdown

### ğŸ”¹ Phase 1 â€” Persistent Memory & Storage

**Goal:** Build a crash-safe, append-only memory system.

What was implemented:
- Disk-backed `MetadataStore`
- Atomic writes to prevent corruption
- Append-only memory (never overwrite)
- Clear separation between:
  - **Source-of-truth memory**
  - **Derived data**
---

### ğŸ”¹ Phase 2 â€” Embeddings & Chunking

#### Phase 2A â€” Chunking Engine

**Goal:** Convert raw memory into reusable, semantically coherent chunks.

What was implemented:
- Deterministic chunking (paragraph-aware)
- Chunk quality rules:
  - A chunk should answer at least one clear question
- Overlap tolerance for context continuity
- Chunk IDs treated as disposable
---

#### Phase 2B â€” Embedding System & Vector Index

**Goal:** Represent chunks numerically for similarity search.

What was implemented:
- Local embedding model abstraction
- Fixed-dimension vectors
- Disk-persistent embedding store
- Model-aware embedding storage
---

### ğŸ”¹ Phase 3 â€” Retrieval Engine

**Goal:** Retrieve relevant chunks with minimal noise.

What was implemented:
- Cosine similarity search
- Top-k retrieval
- Similarity thresholding
- Explicit retrieval states
---

### ğŸ”¹ Phase 4 â€” Knowledge-Grounded Answer Generation

**Goal:** Prevent hallucinations and enforce grounding.

What was implemented:
- Context injection from retrieved chunks
- Answer generation constrained to evidence
- Confidence-aware responses
- Explicit handling of weak or missing evidence
---

### ğŸ”¹ Phase 5 â€” Integration, Ingestion & Cleanup

#### Phase 5A â€” LLM Integration
- Groq API integration
- Environment-based API key loading

#### Phase 5B â€” File Ingestion
- Support for PDF / TXT / MD
- File â†’ memory â†’ chunks â†’ embeddings pipeline
- Deterministic rebuild of derived data

#### Phase 5C â€” CLI Unification
- Single entry point: `main.py`
- Commands:
  - `ingest`
  - `ask`
  - `chat`

#### Phase 5D â€” Cleanup & Finalization
- Removal of legacy runner scripts
- Derived data cleanup (no chunk duplication)
- Versioned release (`v1.0.0`)


## ğŸš€ How to Run the System

### 1ï¸âƒ£ Setup Environment

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
````

Set your Groq API key:

```bash
export GROQ_API_KEY="your_api_key_here"
```

(or use a `.env` file if preferred) inside the .env file put
	`GROQ_API_KEY=your_api_key`

---

### 2ï¸âƒ£ Ingest Files (Learning Mode)

```bash
python main.py ingest /absolute/path/to/file.pdf
```

This is done **once per file**. and once its done its data store into memory can reuse anytime.

---

### 3ï¸âƒ£ Ask Questions (Thinking Mode)

```bash
python main.py ask "Why did my food delivery startup fail?"
```

No re-ingestion. Uses stored knowledge only.

---

### 4ï¸âƒ£ Interactive Chat Mode

```bash
python main.py chat
```

Example:

```
> Summarize the Linux command guide
> What mistakes were mentioned earlier?
> exit(to exit chat mode)
```

---

## ğŸ”’ What This Project Intentionally Does NOT Use

- âŒ LangChain
    
- âŒ LangGraph
    
- âŒ Vector databases
    
- âŒ Tool orchestration frameworks
    

These are avoided **on purpose** so the core mechanics are fully build manually.

---

## ğŸ§  What This Project about(building level)

- Persistent memory architecture
    
- Real RAG 
    
- Chunking strategy design
    
- Embedding trade-offs
    
- Retrieval noise control
    
- Hallucination prevention
    
- System-level thinking for AI agents
    
---

